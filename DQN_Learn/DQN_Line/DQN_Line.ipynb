{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入相关库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "np.random.seed(2)\n",
    "# 伪随机数列，确定的随机种子可以使得随机数列一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# 定义超参数\n",
    "N_STATES = 6\n",
    "# 状态的总数，一个一维世界\n",
    "ACTIONS = ['left', 'right']\n",
    "# 动作\n",
    "EPSILON = 0.9\n",
    "# greedy policy 属于Q_learning的一种方法，即90%选择最优策略，10%不选择\n",
    "ALPHA = 0.1\n",
    "# learning rate\n",
    "LAMBDA = 0.9\n",
    "# 衰减因子，未来的奖励比不上现在的奖励，所以有衰减\n",
    "MAX_EPISODES = 13\n",
    "# 最大回合数\n",
    "FRESH_TIME = 0.3\n",
    "# 走一步的时间，为了方便观察游戏而已"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# 函数定义\n",
    "def build_q_table(n_states, actions):\n",
    "    table = pd.DataFrame(\n",
    "        np.zeros((n_states, len(actions))),\n",
    "        # 初始值，直接在pd.DataFrame中放一个数组\n",
    "        columns=actions,\n",
    "        # 列索引为actions\n",
    "    )\n",
    "    # 用DataFrame存储q表格\n",
    "    return table\n",
    "# 输入状态数、动作，分别作为行、列标，内容填充该状态下做该动作的价值，初始化为0\n",
    "\n",
    "def choose_action(state, q_table):\n",
    "    # 根据所处状态及做出动作带来的奖励进行动作选择\n",
    "    state_actions = q_table.iloc[state, :]\n",
    "    # iloc函数：根据标签的所在位置，从0开始计数，先选取行再选取列\n",
    "    # 实际上就是通过定位取出特定行、列中的内容\n",
    "    if (np.random.uniform() > EPSILON) or (state_actions.all() == 0):\n",
    "        #  numpy.random.uniform(low,high,size)\n",
    "        # low: 采样下界，float类型，默认值为0；\n",
    "        # high: 采样上界，float类型，默认值为1；\n",
    "        # size: 输出样本数目，为int或元组(tuple)类型，例如，size=(m,n,k), 则输出 m * n * k 个样本，缺省时输出1个值。\n",
    "        # or (state_actions.all() == 0),初始化问题，刚开始的时候都是0，这时候随机选择一个动作\n",
    "        action_name = np.random.choice(ACTIONS)\n",
    "        # 随机选择一个动作\n",
    "    else:\n",
    "        # act greedy，选取价值最高的动作\n",
    "        action_name = state_actions.idxmax()\n",
    "        # 原本是.argmax, 从中选取最大的\n",
    "        # 将argmax替换为idxmax，因为argmax在新版本的panda中意味着不同的功能\n",
    "        # idxmax()方法返回轴上最大值第一次出现的索引，是我们自己定义的“left”和“right”\n",
    "        # state_actions中是动作价值，那么我们找到价值最大的那个，它对应的索引就是left或者right，向左或者向右\n",
    "        # .argmax返回的是最大值的int位置，也就是下表，是数字，而不是索引\n",
    "        # 详情见https://blog.csdn.net/m0_37690430/article/details/127185238\n",
    "        # idmax()是返回索引，argmax()是返回下标，注意区别\n",
    "    return action_name\n",
    "\n",
    "def get_env_feedback(S, A):\n",
    "# 输入为现在的状态S和将要进行的动作A\n",
    "    if A == 'right':\n",
    "        if S == N_STATES -2:\n",
    "        # S=4，再向右走即走到了5，找到了宝藏\n",
    "            S_ = 'terminal'\n",
    "            R = 1\n",
    "        else:\n",
    "            S_ = S + 1\n",
    "            R = 0\n",
    "    else:\n",
    "    # 向左走\n",
    "        R = 0\n",
    "        if S == 0:\n",
    "            S_ = S\n",
    "            # 已经在最左边了\n",
    "        else:\n",
    "            S_ = S -1\n",
    "    return S_, R\n",
    "\n",
    "\n",
    "def update_env(S, episode, step_counter):\n",
    "    # This is how environment be updated\n",
    "    env_list = ['-']*(N_STATES-1) + ['T']   # '---------T' our environment\n",
    "    if S == 'terminal':\n",
    "        interaction = 'Episode %s: total_steps = %s' % (episode+1, step_counter)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(2)\n",
    "        print('\\r                                ', end='')\n",
    "    else:\n",
    "        env_list[S] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(FRESH_TIME)\n",
    "\n",
    "# 模型训练主体\n",
    "def rl():\n",
    "    q_table = build_q_table(N_STATES, ACTIONS)\n",
    "    # 初始化一个价值表\n",
    "    for episode in range(MAX_EPISODES):\n",
    "        # 一轮一轮地跑，最多跑MAX_EPISONDES轮\n",
    "        step_counter = 0\n",
    "        # 记录步数\n",
    "        S = 0\n",
    "        # 初始化状态位置\n",
    "        is_terminated = False\n",
    "        # \"结束了\"，作为对本轮游戏是否结束的判断，也是退出循环的标志\n",
    "        update_env(S, episode, step_counter)\n",
    "        while not is_terminated:\n",
    "            # while not，当后面的值是0或者false的时候就会一直循环\n",
    "\n",
    "            A = choose_action(S, q_table)\n",
    "            # 根据现有状态和价值表选择动作\n",
    "            S_, R = get_env_feedback(S, A)\n",
    "            # 根据S做出a后，环境会返回一个新的状态S_以及奖励R\n",
    "            # ###########集齐了s，a，s_,r四元素后就要开始更新DQN了（这里是表格）##########\n",
    "            q_predict = q_table.loc[S, A]\n",
    "            # 可以理解为DQN对状态S下做状态A价值的估计，是我们要更新的量，希望它能更接近真实\n",
    "            ########TD_learning########\n",
    "            if S_  != 'terminal':\n",
    "                # 游戏尚未结束，继续更新参数\n",
    "                q_target = R + LAMBDA * q_table.iloc[S_, :].max()\n",
    "                # R 是环境返回的奖励，即真实价值\n",
    "                # LAMBDA 是衰减因子，后续的奖励预测不及现在的，所以衰减\n",
    "                # q_table.iloc[S_, :].max()，这个是算法推导出来的\n",
    "                # 最优价值函数的关键就在于是在S_t+1时刻的价值预测进行最大化处理(对动作a求最大化)\n",
    "            else:\n",
    "                q_target = R\n",
    "                is_terminated = True\n",
    "                # 这一句会直接跳出while循环，进行下一轮的游戏了\n",
    "                #游戏结束了\n",
    "\n",
    "            # #######更新价值表，即更新DQN网络##########\n",
    "\n",
    "            q_table.loc[S, A] += ALPHA * (q_target - q_predict)\n",
    "            # 目标就是想让价值表对于动作价值的评估更接近于target，进而更接近真实值\n",
    "            S = S_\n",
    "            # 进入下一个状态\n",
    "\n",
    "            update_env(S, episode, step_counter+1)\n",
    "            step_counter += 1\n",
    "    return q_table\n",
    "# 返回训练好的价值表，即DQN神经网络"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                \r\n",
      "Q-table:\n",
      "\n",
      "       left     right\n",
      "0  0.000001  0.005728\n",
      "1  0.000271  0.032612\n",
      "2  0.002454  0.111724\n",
      "3  0.000073  0.343331\n",
      "4  0.000810  0.745813\n",
      "5  0.000000  0.000000\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    q_table = rl()\n",
    "    print('\\r\\nQ-table:\\n')\n",
    "    print(q_table)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}